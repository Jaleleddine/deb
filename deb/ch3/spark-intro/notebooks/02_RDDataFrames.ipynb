{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrames as RDD\n",
    "In the first chapter we learned about Pandas DataFrames and how great they are to work with for data science. Let's see how we can make similar objects with Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL\n",
    "In the previous sheet, we used a basic SparkContext to access our data. But what if we wanted a more robust Spark interface that could interact with SQL, Hive, and other data storage systems? That's where SparkSession comes into the picture. Part of the `pyspark.sql` package, this constructor can be connected to a wide variaty of data sources and returns a SparkSession which can be used to create RDDs. Luckily, one of the data sources is your local machine so we can access the flight csv similarly to how we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sparkql = pyspark.sql.SparkSession.builder.master('local').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `SparkSession.builder` is creating our session\n",
    "- `.master('local')` tells the Session\n",
    "- `getOrCreate()` creates a new Session since none has been created already. The builder can retrieve existing session with this function as well.\n",
    "\n",
    "### Read flights\n",
    "Now that we've got a session, let's read in the same flight data as before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flight_date', 'airline', 'tailnumber', 'flight_number', 'src', 'dest', 'departure_time', 'arrival_time', 'flight_time', 'distance']\n",
      "StructType(List(StructField(flight_date,StringType,true),StructField(airline,StringType,true),StructField(tailnumber,StringType,true),StructField(flight_number,StringType,true),StructField(src,StringType,true),StructField(dest,StringType,true),StructField(departure_time,StringType,true),StructField(arrival_time,StringType,true),StructField(flight_time,StringType,true),StructField(distance,StringType,true)))\n",
      "+-----------+-------+----------+-------------+---+----+--------------+------------+-----------+--------+\n",
      "|flight_date|airline|tailnumber|flight_number|src|dest|departure_time|arrival_time|flight_time|distance|\n",
      "+-----------+-------+----------+-------------+---+----+--------------+------------+-----------+--------+\n",
      "| 2019-11-28|     9E|    N8974C|         3280|CHA| DTW|          1300|        1455|      115.0|   505.0|\n",
      "| 2019-11-28|     9E|    N901XJ|         3281|JAX| RDU|           700|         824|       84.0|   407.0|\n",
      "| 2019-11-28|     9E|    N901XJ|         3282|RDU| LGA|           900|        1039|       99.0|   431.0|\n",
      "+-----------+-------+----------+-------------+---+----+--------------+------------+-----------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_file = '../data/flights.csv'\n",
    "flight_df = sparkql.read.csv(flight_file, header=True)\n",
    "print(flight_df.columns)\n",
    "print(flight_df.schema)\n",
    "flight_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this RDD has a lot of the same properties as a Pandas DataFrame! We get columns and a sechmea and can use `show(n)` to look at `n` rows of the data. We also still have RDD properties like `.count()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12715\n"
     ]
    }
   ],
   "source": [
    "print(flight_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame operation with an RDD\n",
    "This since this `.read.csv()` has created an RDD with DataFrame properties let's try some of the DataFrame operations! They are similar to Pandas options but Pyspark does have some differences\n",
    "### Change column data type\n",
    "As we can see from the `flight_df.schema` since no schema information was supplied, everything was read in as `StringType`. Let's change the date columns to the correct type. First we have to import the desired data type, in the case `DateType`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(flight_date,DateType,true),StructField(airline,StringType,true),StructField(tailnumber,StringType,true),StructField(flight_number,StringType,true),StructField(src,StringType,true),StructField(dest,StringType,true),StructField(departure_time,StringType,true),StructField(arrival_time,StringType,true),StructField(flight_time,StringType,true),StructField(distance,StringType,true)))\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DateType\n",
    "flight_df = flight_df.withColumn('flight_date', flight_df['flight_date'].cast(DateType()))\n",
    "print(flight_df.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These also allows us to see the power of the `withColumn` function. The first argument is the \n",
    "output column name. If the output column name is an existing column in the RDD this allows us to update columns. If it is a new column name, a new column is created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+----------+-------------+---+----+--------------+------------+-----------+--------+--------+\n",
      "|flight_date|airline|tailnumber|flight_number|src|dest|departure_time|arrival_time|flight_time|distance|num_dist|\n",
      "+-----------+-------+----------+-------------+---+----+--------------+------------+-----------+--------+--------+\n",
      "| 2019-11-28|     9E|    N8974C|         3280|CHA| DTW|          1300|        1455|      115.0|   505.0|   505.0|\n",
      "| 2019-11-28|     9E|    N901XJ|         3281|JAX| RDU|           700|         824|       84.0|   407.0|   407.0|\n",
      "| 2019-11-28|     9E|    N901XJ|         3282|RDU| LGA|           900|        1039|       99.0|   431.0|   431.0|\n",
      "+-----------+-------+----------+-------------+---+----+--------------+------------+-----------+--------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "flight_df = flight_df.withColumn('num_dist', flight_df['distance'].cast(DoubleType()))\n",
    "flight_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop columns\n",
    "Similarly, it is easy to drop columns as well. Simply supply the `drop()` function with the names of the columns to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+----------+-------------+---+----+--------------+------------+-----------+--------+\n",
      "|flight_date|airline|tailnumber|flight_number|src|dest|departure_time|arrival_time|flight_time|num_dist|\n",
      "+-----------+-------+----------+-------------+---+----+--------------+------------+-----------+--------+\n",
      "| 2019-11-28|     9E|    N8974C|         3280|CHA| DTW|          1300|        1455|      115.0|   505.0|\n",
      "| 2019-11-28|     9E|    N901XJ|         3281|JAX| RDU|           700|         824|       84.0|   407.0|\n",
      "| 2019-11-28|     9E|    N901XJ|         3282|RDU| LGA|           900|        1039|       99.0|   431.0|\n",
      "+-----------+-------+----------+-------------+---+----+--------------+------------+-----------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_df = flight_df.drop(\"distance\")\n",
    "flight_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you only want to rename a column with no changes, we can use `withColumnRenamed` instead of a full `withColumn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+----------+-------------+---+----+--------------+------------+-----------+-----+\n",
      "|flight_date|airline|tailnumber|flight_number|src|dest|departure_time|arrival_time|flight_time| dist|\n",
      "+-----------+-------+----------+-------------+---+----+--------------+------------+-----------+-----+\n",
      "| 2019-11-28|     9E|    N8974C|         3280|CHA| DTW|          1300|        1455|      115.0|505.0|\n",
      "| 2019-11-28|     9E|    N901XJ|         3281|JAX| RDU|           700|         824|       84.0|407.0|\n",
      "| 2019-11-28|     9E|    N901XJ|         3282|RDU| LGA|           900|        1039|       99.0|431.0|\n",
      "+-----------+-------+----------+-------------+---+----+--------------+------------+-----------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_df = flight_df.withColumnRenamed(\"num_dist\", \"dist\")\n",
    "flight_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection\n",
    "We can also select specific columns to return or fitler rows similar to how we filtered the simple RDDs. If we supply the `select()` the columns we want, a RDD with just those is returned. For `filter()` we can leverage the structure of these RDDs for filtering similar to how we would use `==` to create a Boolean mask in a Pandas DataFrame. For simple comparisons this is more straightforward than creating a lambda function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+----+\n",
      "|flight_date|airline|dest|\n",
      "+-----------+-------+----+\n",
      "| 2019-11-28|     9E| DTW|\n",
      "| 2019-11-28|     9E| RDU|\n",
      "| 2019-11-28|     9E| LGA|\n",
      "+-----------+-------+----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----------+-------+----------+-------------+---+----+--------------+------------+-----------+------+\n",
      "|flight_date|airline|tailnumber|flight_number|src|dest|departure_time|arrival_time|flight_time|  dist|\n",
      "+-----------+-------+----------+-------------+---+----+--------------+------------+-----------+------+\n",
      "| 2019-11-28|     AA|    N832NN|         1402|PDX| PHX|           820|        1153|      153.0|1009.0|\n",
      "| 2019-11-28|     AA|    N939AN|         2298|PDX| ORD|           627|        1229|      242.0|1739.0|\n",
      "| 2019-11-28|     AA|    N992AU|         2577|PDX| DFW|           600|        1141|      221.0|1616.0|\n",
      "+-----------+-------+----------+-------------+---+----+--------------+------------+-----------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----------+-------+----------+-------------+---+----+--------------+------------+-----------+------+\n",
      "|flight_date|airline|tailnumber|flight_number|src|dest|departure_time|arrival_time|flight_time|  dist|\n",
      "+-----------+-------+----------+-------------+---+----+--------------+------------+-----------+------+\n",
      "| 2019-11-28|     AA|    N143AN|         1316|DFW| PDX|           855|        1114|      259.0|1616.0|\n",
      "| 2019-11-28|     AA|    N832NN|         1402|PDX| PHX|           820|        1153|      153.0|1009.0|\n",
      "| 2019-11-28|     AA|    N152UW|         1437|PHX| PDX|          1150|        1345|      175.0|1009.0|\n",
      "+-----------+-------+----------+-------------+---+----+--------------+------------+-----------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_df.select(\"flight_date\", \"airline\", \"dest\").show(3)\n",
    "flight_df.filter(flight_df.src == \"PDX\").show(3)\n",
    "flight_df.filter((flight_df.src == \"PDX\") | (flight_df.dest == \"PDX\")).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation\n",
    "We can also group our data by a column or a set of columns as showin in the two examples bellow. We use the `count()` option to find the number of rows within each grouping. The grouped RDD does not have a view unless we apply some sort of function to each group; here we chose to count the number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|dest|count|\n",
      "+----+-----+\n",
      "| BGM|    1|\n",
      "| PSE|    3|\n",
      "| INL|    1|\n",
      "+----+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----+-------+-----+\n",
      "|dest|airline|count|\n",
      "+----+-------+-----+\n",
      "| MDT|     9E|    1|\n",
      "| KTN|     AS|    4|\n",
      "| JAX|     EV|    2|\n",
      "+----+-------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_df.groupBy(\"dest\").count().show(3)\n",
    "flight_df.groupBy(\"dest\", \"airline\").count().show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
