{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrames as RDD\n",
    "in chapter 1 we learned about Pandas DataFrames and how great they are to work with. Let's see how we can make similar objects with Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL\n",
    "In the previous sheet, we used a basic SparkContext to access our data. But what if we wanted a more robust Spark interface that could interact with SQL, Hive, and other data storage systems? That's where SparkSession comes into the picture. Part of the `pyspark.sql` package, this constructor can be connected to a wide variaty of data sources and returns a SparkSession which can be used to create RDDs. Luckily, one of the data sources is your local machine so we can access the flight csv similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sparkql = pyspark.sql.SparkSession.builder.master('local').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `SparkSession.builder` is creating our session\n",
    "- `.master('local')` tells the Session\n",
    "- `getOrCreate()` creates a new Session since none has been created already. The builder can retrieve existing session with this function as well.\n",
    "\n",
    "### Read flights\n",
    "Now that we've got a session, let's read in the same flight data as before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flight_date', 'airline', 'tailnumber', 'flight_number', 'src', 'dest', 'departure_time', 'arrival_time', 'flight_time', 'distance']\n",
      "StructType(List(StructField(flight_date,StringType,true),StructField(airline,StringType,true),StructField(tailnumber,StringType,true),StructField(flight_number,StringType,true),StructField(src,StringType,true),StructField(dest,StringType,true),StructField(departure_time,StringType,true),StructField(arrival_time,StringType,true),StructField(flight_time,StringType,true),StructField(distance,StringType,true)))\n",
      "+-----------+-------+----------+-------------+---+----+--------------+------------+-----------+--------+\n",
      "|flight_date|airline|tailnumber|flight_number|src|dest|departure_time|arrival_time|flight_time|distance|\n",
      "+-----------+-------+----------+-------------+---+----+--------------+------------+-----------+--------+\n",
      "| 2019-11-28|     9E|    N8974C|         3280|CHA| DTW|          1300|        1455|      115.0|   505.0|\n",
      "| 2019-11-28|     9E|    N901XJ|         3281|JAX| RDU|           700|         824|       84.0|   407.0|\n",
      "| 2019-11-28|     9E|    N901XJ|         3282|RDU| LGA|           900|        1039|       99.0|   431.0|\n",
      "+-----------+-------+----------+-------------+---+----+--------------+------------+-----------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_file = '../data/flights.csv'\n",
    "flight_df = sparkql.read.csv(flight_file, header=True)\n",
    "print(flight_df.columns)\n",
    "print(flight_df.schema)\n",
    "flight_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12715\n"
     ]
    }
   ],
   "source": [
    "print(flight_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame operation with an RDD\n",
    "The syntax is very similar but now we are using the `.read.csv` method which functions very similar to the `read_csv()` method from `pandas` creating an RDD with DataFrame-like properties as we can see from the `schema`, `columns` and `head` above. As such, let try some of the DataFrame operations!\n",
    "### Change column data type\n",
    "As we can see from the `flight_df.schema` since no schema information was supplied, everything was read in as `StringType`. Let's change the date columns to the correct type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(flight_date,DateType,true),StructField(airline,StringType,true),StructField(tailnumber,StringType,true),StructField(flight_number,StringType,true),StructField(src,StringType,true),StructField(dest,StringType,true),StructField(departure_time,StringType,true),StructField(arrival_time,StringType,true),StructField(flight_time,StringType,true),StructField(distance,StringType,true)))\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DateType\n",
    "flight_df = flight_df.withColumn('flight_date', flight_df['flight_date'].cast(DateType()))\n",
    "print(flight_df.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These also allows us to see the power of the `withColumn` function. the first argument is the \n",
    "output column name. If this is an existing column in the RDD this allows us to update columns. If it is a new column name, a new column is created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+----------+-------------+---+----+--------------+------------+-----------+--------+--------+\n",
      "|flight_date|airline|tailnumber|flight_number|src|dest|departure_time|arrival_time|flight_time|distance|num_dist|\n",
      "+-----------+-------+----------+-------------+---+----+--------------+------------+-----------+--------+--------+\n",
      "| 2019-11-28|     9E|    N8974C|         3280|CHA| DTW|          1300|        1455|      115.0|   505.0|   505.0|\n",
      "| 2019-11-28|     9E|    N901XJ|         3281|JAX| RDU|           700|         824|       84.0|   407.0|   407.0|\n",
      "| 2019-11-28|     9E|    N901XJ|         3282|RDU| LGA|           900|        1039|       99.0|   431.0|   431.0|\n",
      "+-----------+-------+----------+-------------+---+----+--------------+------------+-----------+--------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "flight_df = flight_df.withColumn('num_dist', flight_df['distance'].cast(DoubleType()))\n",
    "flight_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, it is easy to drop columns as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+----------+-------------+---+----+--------------+------------+-----------+--------+\n",
      "|flight_date|airline|tailnumber|flight_number|src|dest|departure_time|arrival_time|flight_time|num_dist|\n",
      "+-----------+-------+----------+-------------+---+----+--------------+------------+-----------+--------+\n",
      "| 2019-11-28|     9E|    N8974C|         3280|CHA| DTW|          1300|        1455|      115.0|   505.0|\n",
      "| 2019-11-28|     9E|    N901XJ|         3281|JAX| RDU|           700|         824|       84.0|   407.0|\n",
      "| 2019-11-28|     9E|    N901XJ|         3282|RDU| LGA|           900|        1039|       99.0|   431.0|\n",
      "+-----------+-------+----------+-------------+---+----+--------------+------------+-----------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_df = flight_df.drop(\"distance\")\n",
    "flight_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you only want to rename a column with no changes, we can use `withColumnRenamed` instead of a full `withColumn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+----------+-------------+---+----+--------------+------------+-----------+-----+\n",
      "|flight_date|airline|tailnumber|flight_number|src|dest|departure_time|arrival_time|flight_time| dist|\n",
      "+-----------+-------+----------+-------------+---+----+--------------+------------+-----------+-----+\n",
      "| 2019-11-28|     9E|    N8974C|         3280|CHA| DTW|          1300|        1455|      115.0|505.0|\n",
      "| 2019-11-28|     9E|    N901XJ|         3281|JAX| RDU|           700|         824|       84.0|407.0|\n",
      "| 2019-11-28|     9E|    N901XJ|         3282|RDU| LGA|           900|        1039|       99.0|431.0|\n",
      "+-----------+-------+----------+-------------+---+----+--------------+------------+-----------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_df = flight_df.withColumnRenamed(\"num_dist\", \"dist\")\n",
    "flight_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection\n",
    "We can also select specific columns to return or fitler rows similar to how we filtered the simple RDDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+----+\n",
      "|flight_date|airline|dest|\n",
      "+-----------+-------+----+\n",
      "| 2019-11-28|     9E| DTW|\n",
      "| 2019-11-28|     9E| RDU|\n",
      "| 2019-11-28|     9E| LGA|\n",
      "+-----------+-------+----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----------+-------+----------+-------------+---+----+--------------+------------+-----------+------+\n",
      "|flight_date|airline|tailnumber|flight_number|src|dest|departure_time|arrival_time|flight_time|  dist|\n",
      "+-----------+-------+----------+-------------+---+----+--------------+------------+-----------+------+\n",
      "| 2019-11-28|     AA|    N832NN|         1402|PDX| PHX|           820|        1153|      153.0|1009.0|\n",
      "| 2019-11-28|     AA|    N939AN|         2298|PDX| ORD|           627|        1229|      242.0|1739.0|\n",
      "| 2019-11-28|     AA|    N992AU|         2577|PDX| DFW|           600|        1141|      221.0|1616.0|\n",
      "+-----------+-------+----------+-------------+---+----+--------------+------------+-----------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----------+-------+----------+-------------+---+----+--------------+------------+-----------+------+\n",
      "|flight_date|airline|tailnumber|flight_number|src|dest|departure_time|arrival_time|flight_time|  dist|\n",
      "+-----------+-------+----------+-------------+---+----+--------------+------------+-----------+------+\n",
      "| 2019-11-28|     AA|    N143AN|         1316|DFW| PDX|           855|        1114|      259.0|1616.0|\n",
      "| 2019-11-28|     AA|    N832NN|         1402|PDX| PHX|           820|        1153|      153.0|1009.0|\n",
      "| 2019-11-28|     AA|    N152UW|         1437|PHX| PDX|          1150|        1345|      175.0|1009.0|\n",
      "+-----------+-------+----------+-------------+---+----+--------------+------------+-----------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_df.select(\"flight_date\", \"airline\", \"dest\").show(3)\n",
    "flight_df.filter(flight_df.src == \"PDX\").show(3)\n",
    "flight_df.filter((flight_df.src == \"PDX\") | (flight_df.dest == \"PDX\")).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation\n",
    "Count arrivals per airport. Then arrivals per airline per airport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|dest|count|\n",
      "+----+-----+\n",
      "| BGM|    1|\n",
      "| PSE|    3|\n",
      "| INL|    1|\n",
      "+----+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----+-------+-----+\n",
      "|dest|airline|count|\n",
      "+----+-------+-----+\n",
      "| MDT|     9E|    1|\n",
      "| KTN|     AS|    4|\n",
      "| JAX|     EV|    2|\n",
      "+----+-------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_df.groupBy(\"dest\").count().show(3)\n",
    "flight_df.groupBy(\"dest\", \"airline\").count().show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
