{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Spark #\n",
    "## What is Spark?\n",
    "Basic info about Apache Spark\n",
    "## SparkContext: how you interact with your data\n",
    "A SparkContext is the most basic tool for connecting to your data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Datasets (RDD)\n",
    "Now that we can interact with our data using a SparkSession/context, what happens to that data? We put it in RDDs! There is a lot going on under the hood with RDDs that leverage the distrubuted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:262\n",
      "[('PDX', 1), ('LAX', 5), ('DEN', 3), ('PDX', 2), ('JFK', 9), ('DEN', 5), ('PDX', 7), ('JFK', 10)]\n",
      "8\n",
      "[('PDX', 1), ('PDX', 2), ('PDX', 7)]\n",
      "[('JFK', <pyspark.resultiterable.ResultIterable object at 0x7f6cad91de20>), ('LAX', <pyspark.resultiterable.ResultIterable object at 0x7f6cad91de80>), ('DEN', <pyspark.resultiterable.ResultIterable object at 0x7f6cad91dfa0>), ('PDX', <pyspark.resultiterable.ResultIterable object at 0x7f6cad92b040>)]\n",
      "[('JFK', <pyspark.resultiterable.ResultIterable object at 0x7f6cad91dfa0>), ('LAX', <pyspark.resultiterable.ResultIterable object at 0x7f6c9615ae80>), ('DEN', <pyspark.resultiterable.ResultIterable object at 0x7f6c9615a3d0>), ('PDX', <pyspark.resultiterable.ResultIterable object at 0x7f6cad92b0a0>)]\n",
      "[('JFK', 19), ('LAX', 5), ('DEN', 8), ('PDX', 10)]\n",
      "('DEN', 3)\n",
      "[('DEN', 3), ('DEN', 5), ('JFK', 9), ('JFK', 10), ('LAX', 5), ('PDX', 1), ('PDX', 2), ('PDX', 7)]\n"
     ]
    }
   ],
   "source": [
    "list_of_arrivals = [\n",
    "    (\"PDX\", 1),\n",
    "    (\"LAX\", 5),\n",
    "    (\"DEN\", 3),\n",
    "    (\"PDX\", 2),\n",
    "    (\"JFK\", 9),\n",
    "    (\"DEN\", 5),\n",
    "    (\"PDX\", 7),\n",
    "    (\"JFK\", 10),\n",
    "]\n",
    "arrivals_rdd = sc.parallelize(list_of_arrivals)\n",
    "print(arrivals_rdd)\n",
    "print(arrivals_rdd.collect())\n",
    "print(arrivals_rdd.count())\n",
    "pdx_arrivals = arrivals_rdd.filter(lambda x: x[0] == \"PDX\")\n",
    "print(pdx_arrivals.collect())\n",
    "grouped_arrivals = arrivals_rdd.groupByKey()\n",
    "print(grouped_arrivals.collect())\n",
    "print(grouped_arrivals.collect())\n",
    "grouped_arrivals = grouped_arrivals.mapValues(sum)\n",
    "print(grouped_arrivals.collect())\n",
    "sorted_arrivals = arrivals_rdd.sortByKey()\n",
    "print(sorted_arrivals.first())\n",
    "print(sorted_arrivals.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 12716 flights!\n",
      "Of these flights, 270 involved PDX\n"
     ]
    }
   ],
   "source": [
    "flight_file = '../data/flights.csv'\n",
    "txt = sc.textFile(flight_file)\n",
    "print(\"We have {} flights!\".format(txt.count()))\n",
    "\n",
    "pdx_lines = txt.filter(lambda line: 'pdx' in line.lower())\n",
    "print(\"Of these flights, {} involved PDX\".format(pdx_lines.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['flight_date',\n",
       "  'airline',\n",
       "  'tailnumber',\n",
       "  'flight_number',\n",
       "  'src',\n",
       "  'dest',\n",
       "  'departure_time',\n",
       "  'arrival_time',\n",
       "  'flight_time',\n",
       "  'distance'],\n",
       " ['2019-11-28',\n",
       "  '9E',\n",
       "  'N8974C',\n",
       "  '3280',\n",
       "  'CHA',\n",
       "  'DTW',\n",
       "  '1300',\n",
       "  '1455',\n",
       "  '115.0',\n",
       "  '505.0']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv = txt.map(lambda x: x.split(','))\n",
    "csv.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, as you can probably guess, this is not the most efficient way to deal with CSV data. Luckily, the PysparkSQL module will give us a much better toolset for tabular data which we will explore in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
